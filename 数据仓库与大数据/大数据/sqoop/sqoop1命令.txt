

--
 ==== sqoop 是一个用于将关系型数据导入至HDFS，或将HDFS数据导出至关系型数据库 =======
--

1.查看版本
sqoop version

2.查看帮助信息
sqoop help

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

See 'sqoop help COMMAND' for information on a specific command.

使用'sqoop help COMMAND'查看某个命令具体的用法。

3.查看数据库
sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root -password root


4.查看某个库下的表
	如hive库中的表：
sqoop list-tables --connect jdbc:mysql://localhost:3306/hive --username root -password root


5.sqoop metastore
	metastore服务用于共享job任务，使不同用户可以使用其他用户所创建的job。如果不启动metastore，
则各自只能用各自私人仓库下（默认在$HOME/.sqoop/）的job。
启动：sqoop metastore &
启动metastore服务后，使用jps命令可以看到一个sqoop服务。
停止：sqoop metastore -shutdown


6.import
	将关系型数据导入hdfs，sqoop会起多个mapreduce作业来读取数据，每个map任务是按表的主键进行切分的。
可以使用参数split by 指定切分列。

sqoop import --connect jdbc:mysql://localhost:3306/hive --table FUNCS --username root -password root 
--driver com.mysql.jdbc.Driver --bindir ./
其他参数：    
--hive-import可以直接将数据导入至hive中。	
-m N 可以指定N个并发。

从日志中看到，map 100% reduce 0%，说明这个抽取数据的任务只有map段。
在目录中会看到一个part-m-00000的文件，说明已经成功导入。
通过hdfs dfs -cat /user/hadoop/TBLS/part-m-00000，进行数据的简单查看。

6.2 import-all-tables
将数据库的表全部导入至HDFS.

7.create-hive-table

	使用此命令可以直接创建hive表,不过还是建议在hive中建表
   --hive-home <dir>                           Override $HIVE_HOME
   --hive-overwrite                            Overwrite existing data in
                                               the Hive table
   --hive-table <table-name>                   Sets the table name to use
                                               when importing to hive

sqoop create-hive-table --connect jdbc:mysql://localhost:3306/hive --driver com.mysql.jdbc.Driver --table TBLS \ --fields-terminated-by ',' --username root -P


8. eval
eval可以执行对应数据库的SQL语句
sqoop eval --connect jdbc:mysql://localhost:3306/hive --driver com.mysql.jdbc.Driver --query "select * from DBS"\
 --username root --password root


9.export
通常用于将hive的统计结果导回关系型数据库，以便进一步的查询与应用。

sqoop  export --connect jdbc:mysql://localhost:3306/hive --driver com.mysql.jdbc.Driver --bindir ./
 --table student3 --export-dir /user/hive/warehouse/test.db/student3 --username root -P

其他参数：
 -m 指定并行度
 -staging-table 指定中转表
 
10.job
创建及运行sqoop job的命令
sqoop job -create [-delete|-exec|-list] -verbose -show

显示job列表
sqoop job -list

创建job
sqoop job -create -- [sqoop命令]，如
sqoop job -create -- list-tables --connect jdbc:mysql://localhost:3306/hive --username root -password root				   